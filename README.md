# Attention Head Probing in Transformers
## ðŸ§  Introduction  

Transformers have achieved state-of-the-art results across NLP, yet their inner workings remain partly opaque.  
One central question is whether **attention heads** in models like BERT perform distinct, interpretable functions or act redundantly.  

This project explores attention head specialization in **BERT-base** by:  
- Analyzing attention patterns across layers and heads.  
- Visualizing linguistic structures such as syntax and semantic relations.  
- Conducting head-masking ablation experiments to quantify each headâ€™s contribution.  

The findings provide insights into the **redundancy and specialization** of attention heads, offering a step toward greater interpretability in transformer architectures.  
